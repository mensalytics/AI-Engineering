Building a DataWarehouse in Google Cloud Platform 

Google Cloud Storage & Bigquery 

BIGQUERY ESSENTIALS 
1. STORAGE 
2. PROCESSING 
3. METADATA 
4. SQL INTERFACE 
DATA IS STORAGE IN A DISTRIBUTED FILE SYSTEM CALLED GOOGLE COLOSSUS IN A COLUMNAR STORAGE FORMAT. 
THIS IS A SUCCESSOR TO GOOGLE FILE SYSTEM WHICH IS AN INSPIRATION FOR HADOOP FILE SYSTEM 
ACCESS TO DATA USING METADATA(TABLES) AND SQL INTERFACE TO PROCESS THE DATA 

HOW DOES BIGQUERY PROCESS DATA ? IN A DISTRIBUTED SQL EXECUTION ENGINE INSPIRED BY DREMEL SQL. 


BIGQUERY DATA LOCATION ?? 
IT IS PHYSICALLY LOCATED IN DIFFERENT COUNTRIES AND CITIES 
LOCATIONS ARE GROUPED INTO REGIONS 
REGIONS ARE GROUPED INTO ZONES 
IE ASIA-SOUTHEAST REGION 
TWO ZONES IN THIS REGION : SINGAPORE & JAKARTA 

WHEN I CHOOSE JAKARTA, THE DATA WILL BE PROCESSED AND STORED IN JAKARTA 

WHAT IS BIGQUERY GOOD FOR ?? 
STORING LARGE VOLUMES OF DATA AND PERFORMING ANALYSIS AND PROCESSING USING SQL INTERFACE. 

BIGQUERY CONSOLE 
CREATING DATASETS 
DIFFERENT WAYS TO DO IT ? 
1. USING THE CONSOLE 
A. CREATE A DATASET 
CHECK THE DATASET INFORMATION 
B. CREATE TABLE 
IF NO DATA, THEN USE PUBLIC DATASET IN BIGQUERY 
- INSIDE THE BIGQUERY CONSOLE 
- ADD DATA 
- SEARCH FOR PUBLIC DATASETS 

ONE FEATURE ABOUT BIGQUERY IS TIMESTAMP DATA 
TIMESTAMP DATA STORES INFORMATION ABOUT THE DATE AND TIME IN DETAIL. 
TIME STAMP IS STORED IN UTC FORMAT 
WHICH IS FOR CONSISTENCY 
USEFUL FOR ORGANISATION WHO CONDUCT BUSINESS IN MULTIPLE COUNTRIES OR REGIONS THAT HAVE TIME DIFFERENCES. 
CONVERT TO LOCAL TIMEZONE IN YOUR QUERY 
-- AN EXAMPLE 
SELECT DATETIME(startTime, "America/Los_Angeles") as startTime 
FROM `bigquery-public-data.baseball.games_wide`
- QUERY ABOVE WILL GIVE YOU STARTTIME COLUMN DATA IN LOS ANGELES TIMEZONE 

PREPARING THE INGREDIENTS FOR BUILDING DATAWAREHOUSE 
TOOLS TO USE 
1. ACCESS THE CLOUD SHELL 
2. CHECK CREDENTIALS USING gcloud info 
3. INITIALISE OUR CREDENTIALS USING gcloud init 
4. DOWNLOAD example code and dataset from github 
4.1 - write the code, upload to github and pull code from github to bigquery console explorer 

gcloud init command 
- 2. create a new configuation = eng-config 
- 3. choose default email signed in 
- 4. choose projectid 


TASK 2 - UPLOAD DATA TO GCS VIA GIT 
- CREATE A GCS BUCKET 
- Name it: eng-data-bucket 
- choose multi region as the location 
- data will be stored in STANDARD STORAGE = best for short term storage and frequently assessed data 
- other options 
Nearline => best for backups and data accessed less than once a month 
Coldline => best for disaster recovery and data accessed less than once a month 
Archive => Best for long term digital preservation of data accessed less than once a month 
- Access to objects : prevent internet from accessing it 
choose uniform 
Ensure uniform access to all objects in the bucket by using only bucket-level permissions (IAM). 
This option becomes permanent after 90 days
- Data Protection: choose how to protect the data to prevent data loss 
option 1: None 
option 2: Object versioning(best for data recovery)
option 3: Retention policy(best for compliance) 

- TASK 3: UPLOADING THE LOCAL FILE TO GOOGLE CLOUD STORAGE BUCKET 
gsutil cp -r DataEngineering/EngBook/dataset/* gs://eng-data-bucket

- TASK 4: PRACTICING DEVELOPING A DATAWAREHOUSE 
SCENARIOS 
1. As a regional manager user, I want to know the top two region IDs, 
ordered by the total capacity of the stations in that region.

2. As a regional manager, I want to download the answers 
to my questions as CSV files to my local computer.

3. The data source table is the station table, which is located in the CloudSQL-MySQL database.

4. There will be more data sources in the future besides the station table.

THINKING !!!!!
1. WHAT WILL YOU DO ?? 
2. WHAT SERVICES WILL YOU USE ?? 
3. HOW WILL YOU DO IT ? 

SCENARIO 1 
1. Since there is a very specific business rule, we need some transformation. The rule
seems doable in SQL, and BigQuery is a good system for handling such jobs.

2. The BigQuery table can easily be downloaded into CSV files from the console. 
So, storing the results as a BigQuery table or view will be great.
Now the source data is in mysql database and may potentially come from another database in
the future. 

Planning 
- Specific business rules so will need to do some transformations 
this can be done in SQL and bigquery is a good system for handling jobs like that 
- Bigquery table can be easily be downloaded into CSV file from the console 
so it makes sense to store the data in bigquery 

Planning 2 
Data is in CloudSQL - MYSQL database so we need to find a way to extract it from the database and 
load it into bigquery. 
Different options 
a. for standardisation and scalability 
we will use GCS as staging layer from mysql. 
This extraction method applies to any data source 
FINAL 
CREATE MYSQL DATABASE -> EXTRACT MYSQL TO GCS -> LOAD GCS TO BIGQUERY -> CREATE BIGQUERY DATAMART 

STEP 1: LETS CREATE A MYSQL DATABASE 
A. CREATE A CLOUD INSTANCE 
TWO WAYS TO CREATE A CLOUD SQL INSTANCE 
1. EITHER USING THE UI OR USING THE CLOUD SHELL 
WE WILL THE CLOUD SHELL FOR THIS 

B. CONNECT TO THE MYSQL INSTANCE 
To connect to MYSQL INSTANCE USING MYSQL SHELL 
- CODE IS 
gcloud sql connect mysql-instance-source --user=root 
WHEN PROMPTED FOR A PASSWORD it is packt123 
C. CREATE A MYSQL DATABASE 
CODE IS 
CREATE DATABASE apps_db; 
SHOW DATABASE; 

D. CREATE A TABLE IN MYSQL DATABASE 
-- CODE IS 
CREATE TABLE apps_db.stations (
    station_id varchar(255), 
    name varchar(255), 
    region_id varchar(10), 
    capacity integer 
); 
E. IMPORT CSV DATA INTO MYSQL DATABASE 
TO IMPORT THE DATA
- IMPORT BUTTON 
- CHOOSE NAME OF BUCKET AND FILE NAME (STATIONS.CSV)
- FILE FORMAT IS CSV 
- INPUT DESTINATION DATABASE IS apps_db 
- TABLE NAME IS stations 
- CLICK IMPORT BUTTON 


TASK 2: EXTRACT DATA FROM MYSQL TO GCS 
handling Identity and Access Management(IAM) 
assign the cloudsql service account a storage object admin role 
Cloud sql automatically generate once service account to operate 
service account can be seen in the cloud sql console 
service account is autogenerated and different each time you create a cloud sql instance 
To add a new role to the service account 
A. GO TO THE NAVIGATION BAR 
B. CHOOSE IAM & ADMIN -> IAM 
C. CLICK ADD 
D. PASTE THE CLOUD SQL SERVICE ACCOUNT INTO NEW PRINCIPLES 
E. SELECT ROLE 
The role is Storage Object Admin = full control of GCS object
This allows cloudsql service account to write and delete file objects in all GCS buckets in project. 
CLOUDSQL SERVICE ACCOUNT => p612788289633-9a0tvq@gcp-sa-cloud-sql.iam.gserviceaccount.com
This allows service account to load data into GCS bucket 
Now to load the data, access cloud shell ahain and trigger gcloud command to export 
MYSQL query results to a csv file using shell script 




