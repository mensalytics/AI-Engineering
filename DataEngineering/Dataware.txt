Building a DataWarehouse in Google Cloud Platform 

Google Cloud Storage & Bigquery 

BIGQUERY ESSENTIALS 
1. STORAGE 
2. PROCESSING 
3. METADATA 
4. SQL INTERFACE 
DATA IS STORAGE IN A DISTRIBUTED FILE SYSTEM CALLED GOOGLE COLOSSUS IN A COLUMNAR STORAGE FORMAT. 
THIS IS A SUCCESSOR TO GOOGLE FILE SYSTEM WHICH IS AN INSPIRATION FOR HADOOP FILE SYSTEM 
ACCESS TO DATA USING METADATA(TABLES) AND SQL INTERFACE TO PROCESS THE DATA 

HOW DOES BIGQUERY PROCESS DATA ? IN A DISTRIBUTED SQL EXECUTION ENGINE INSPIRED BY DREMEL SQL. 


BIGQUERY DATA LOCATION ?? 
IT IS PHYSICALLY LOCATED IN DIFFERENT COUNTRIES AND CITIES 
LOCATIONS ARE GROUPED INTO REGIONS 
REGIONS ARE GROUPED INTO ZONES 
IE ASIA-SOUTHEAST REGION 
TWO ZONES IN THIS REGION : SINGAPORE & JAKARTA 

WHEN I CHOOSE JAKARTA, THE DATA WILL BE PROCESSED AND STORED IN JAKARTA 

WHAT IS BIGQUERY GOOD FOR ?? 
STORING LARGE VOLUMES OF DATA AND PERFORMING ANALYSIS AND PROCESSING USING SQL INTERFACE. 

BIGQUERY CONSOLE 
CREATING DATASETS 
DIFFERENT WAYS TO DO IT ? 
1. USING THE CONSOLE 
A. CREATE A DATASET 
CHECK THE DATASET INFORMATION 
B. CREATE TABLE 
IF NO DATA, THEN USE PUBLIC DATASET IN BIGQUERY 
- INSIDE THE BIGQUERY CONSOLE 
- ADD DATA 
- SEARCH FOR PUBLIC DATASETS 

ONE FEATURE ABOUT BIGQUERY IS TIMESTAMP DATA 
TIMESTAMP DATA STORES INFORMATION ABOUT THE DATE AND TIME IN DETAIL. 
TIME STAMP IS STORED IN UTC FORMAT 
WHICH IS FOR CONSISTENCY 
USEFUL FOR ORGANISATION WHO CONDUCT BUSINESS IN MULTIPLE COUNTRIES OR REGIONS THAT HAVE TIME DIFFERENCES. 
CONVERT TO LOCAL TIMEZONE IN YOUR QUERY 
-- AN EXAMPLE 
SELECT DATETIME(startTime, "America/Los_Angeles") as startTime 
FROM `bigquery-public-data.baseball.games_wide`
- QUERY ABOVE WILL GIVE YOU STARTTIME COLUMN DATA IN LOS ANGELES TIMEZONE 

PREPARING THE INGREDIENTS FOR BUILDING DATAWAREHOUSE 
TOOLS TO USE 
1. ACCESS THE CLOUD SHELL 
2. CHECK CREDENTIALS USING gcloud info 
3. INITIALISE OUR CREDENTIALS USING gcloud init 
4. DOWNLOAD example code and dataset from github 
4.1 - write the code, upload to github and pull code from github to bigquery console explorer 

gcloud init command 
- 2. create a new configuation = eng-config 
- 3. choose default email signed in 
- 4. choose projectid 


TASK 2 - UPLOAD DATA TO GCS VIA GIT 
- CREATE A GCS BUCKET 
- Name it: eng-data-bucket 
- choose multi region as the location 
- data will be stored in standard storage = best for short term storage and frequently assessed data 
- other options 
Nearline => best for backups and data accessed less than once a month 
Coldline => best for disaster recovery and data accessed less than once a month 
Archive => Best for long term digital preservation of data accessed less than once a month 
- Access to objects : prevent internet from accessing it 
choose uniform 
Ensure uniform access to all objects in the bucket by using only bucket-level permissions (IAM). 
This option becomes permanent after 90 days
- Data Protection: choose how to protect the data to prevent data loss 
option 1: None 
option 2: Object versioning(best for data recovery)
option 3: Retention policy(best for compliance) 

- TASK 3: UPLOADING THE LOCAL FILE TO GOOGLE CLOUD STORAGE BUCKET 

