Building a DataWarehouse in Google Cloud Platform 

Google Cloud Storage & Bigquery 

BIGQUERY ESSENTIALS 
1. STORAGE 
2. PROCESSING 
3. METADATA 
4. SQL INTERFACE 
DATA IS STORAGE IN A DISTRIBUTED FILE SYSTEM CALLED GOOGLE COLOSSUS IN A COLUMNAR STORAGE FORMAT. 
THIS IS A SUCCESSOR TO GOOGLE FILE SYSTEM WHICH IS AN INSPIRATION FOR HADOOP FILE SYSTEM 
ACCESS TO DATA USING METADATA(TABLES) AND SQL INTERFACE TO PROCESS THE DATA 

HOW DOES BIGQUERY PROCESS DATA ? IN A DISTRIBUTED SQL EXECUTION ENGINE INSPIRED BY DREMEL SQL. 


BIGQUERY DATA LOCATION ?? 
IT IS PHYSICALLY LOCATED IN DIFFERENT COUNTRIES AND CITIES 
LOCATIONS ARE GROUPED INTO REGIONS 
REGIONS ARE GROUPED INTO ZONES 
IE ASIA-SOUTHEAST REGION 
TWO ZONES IN THIS REGION : SINGAPORE & JAKARTA 

WHEN I CHOOSE JAKARTA, THE DATA WILL BE PROCESSED AND STORED IN JAKARTA 

WHAT IS BIGQUERY GOOD FOR ?? 
STORING LARGE VOLUMES OF DATA AND PERFORMING ANALYSIS AND PROCESSING USING SQL INTERFACE. 

BIGQUERY CONSOLE 
CREATING DATASETS 
DIFFERENT WAYS TO DO IT ? 
1. USING THE CONSOLE 
A. CREATE A DATASET 
CHECK THE DATASET INFORMATION 
B. CREATE TABLE 
IF NO DATA, THEN USE PUBLIC DATASET IN BIGQUERY 
- INSIDE THE BIGQUERY CONSOLE 
- ADD DATA 
- SEARCH FOR PUBLIC DATASETS 

ONE FEATURE ABOUT BIGQUERY IS TIMESTAMP DATA 
TIMESTAMP DATA STORES INFORMATION ABOUT THE DATE AND TIME IN DETAIL. 
TIME STAMP IS STORED IN UTC FORMAT 
WHICH IS FOR CONSISTENCY 
USEFUL FOR ORGANISATION WHO CONDUCT BUSINESS IN MULTIPLE COUNTRIES OR REGIONS THAT HAVE TIME DIFFERENCES. 
CONVERT TO LOCAL TIMEZONE IN YOUR QUERY 
-- AN EXAMPLE 
SELECT DATETIME(startTime, "America/Los_Angeles") as startTime 
FROM `bigquery-public-data.baseball.games_wide`
- QUERY ABOVE WILL GIVE YOU STARTTIME COLUMN DATA IN LOS ANGELES TIMEZONE 

PREPARING THE INGREDIENTS FOR BUILDING DATAWAREHOUSE 
TOOLS TO USE 
1. ACCESS THE CLOUD SHELL 
2. CHECK CREDENTIALS USING gcloud info 
3. INITIALISE OUR CREDENTIALS USING gcloud init 
4. DOWNLOAD example code and dataset from github 
4.1 - write the code, upload to github and pull code from github to bigquery console explorer 

gcloud init command 
- 2. create a new configuation = eng-config 
- 3. choose default email signed in 
- 4. choose projectid 


TASK 2 - UPLOAD DATA TO GCS VIA GIT 
- CREATE A GCS BUCKET 
- Name it: eng-data-bucket 
- choose multi region as the location 
- data will be stored in STANDARD STORAGE = best for short term storage and frequently assessed data 
- other options 
Nearline => best for backups and data accessed less than once a month 
Coldline => best for disaster recovery and data accessed less than once a month 
Archive => Best for long term digital preservation of data accessed less than once a month 
- Access to objects : prevent internet from accessing it 
choose uniform 
Ensure uniform access to all objects in the bucket by using only bucket-level permissions (IAM). 
This option becomes permanent after 90 days
- Data Protection: choose how to protect the data to prevent data loss 
option 1: None 
option 2: Object versioning(best for data recovery)
option 3: Retention policy(best for compliance) 

- TASK 3: UPLOADING THE LOCAL FILE TO GOOGLE CLOUD STORAGE BUCKET 
gsutil cp -r DataEngineering/EngBook/dataset/* gs://eng-data-bucket

- TASK 4: PRACTICING DEVELOPING A DATAWAREHOUSE 
SCENARIOS 
1. As a regional manager user, I want to know the top two region IDs, 
ordered by the total capacity of the stations in that region.

2. As a regional manager, I want to download the answers 
to my questions as CSV files to my local computer.

3. The data source table is the station table, which is located in the CloudSQL-MySQL database.

4. There will be more data sources in the future besides the station table.

THINKING !!!!!
1. WHAT WILL YOU DO ?? 
2. WHAT SERVICES WILL YOU USE ?? 
3. HOW WILL YOU DO IT ? 

SCENARIO 1 
1. Since there is a very specific business rule, we need some transformation. The rule
seems doable in SQL, and BigQuery is a good system for handling such jobs.

2. The BigQuery table can easily be downloaded into CSV files from the console. 
So, storing the results as a BigQuery table or view will be great.
Now the source data is in mysql database and may potentially come from another database in
the future. 

Planning 
- Specific business rules so will need to do some transformations 
this can be done in SQL and bigquery is a good system for handling jobs like that 
- Bigquery table can be easily be downloaded into CSV file from the console 
so it makes sense to store the data in bigquery 

Planning 2 
Data is in CloudSQL - MYSQL database so we need to find a way to extract it from the database and 
load it into bigquery. 
Different options 
a. for standardisation and scalability 
we will use GCS as staging layer from mysql. 
This extraction method applies to any data source 
FINAL 
CREATE MYSQL DATABASE -> EXTRACT MYSQL TO GCS -> LOAD GCS TO BIGQUERY -> CREATE BIGQUERY DATAMART 

STEP 1: LETS CREATE A MYSQL DATABASE 
A. CREATE A CLOUD INSTANCE 
TWO WAYS TO CREATE A CLOUD SQL INSTANCE 
1. EITHER USING THE UI OR USING THE CLOUD SHELL 
WE WILL THE CLOUD SHELL FOR THIS 

B. CONNECT TO THE MYSQL INSTANCE 
To connect to MYSQL INSTANCE USING MYSQL SHELL 
- CODE IS 
gcloud sql connect mysql-instance-source --user=root 
WHEN PROMPTED FOR A PASSWORD it is packt123 
C. CREATE A MYSQL DATABASE 
CODE IS 
CREATE DATABASE apps_db; 
SHOW DATABASE; 

D. CREATE A TABLE IN MYSQL DATABASE 
-- CODE IS 
CREATE TABLE apps_db.stations (
    station_id varchar(255), 
    name varchar(255), 
    region_id varchar(10), 
    capacity integer 
); 
E. IMPORT CSV DATA INTO MYSQL DATABASE 
TO IMPORT THE DATA
- IMPORT BUTTON 
- CHOOSE NAME OF BUCKET AND FILE NAME (STATIONS.CSV)
- FILE FORMAT IS CSV 
- INPUT DESTINATION DATABASE IS apps_db 
- TABLE NAME IS stations 
- CLICK IMPORT BUTTON 


TASK 2: EXTRACT DATA FROM MYSQL TO GCS 
handling Identity and Access Management(IAM) 
assign the cloudsql service account a storage object admin role 
Cloud sql automatically generate once service account to operate 
service account can be seen in the cloud sql console 
service account is autogenerated and different each time you create a cloud sql instance 
To add a new role to the service account 
A. GO TO THE NAVIGATION BAR 
B. CHOOSE IAM & ADMIN -> IAM 
C. CLICK ADD 
D. PASTE THE CLOUD SQL SERVICE ACCOUNT INTO NEW PRINCIPLES 
E. SELECT ROLE 
The role is Storage Object Admin = full control of GCS object
This allows cloudsql service account to write and delete file objects in all GCS buckets in project. 
CLOUDSQL SERVICE ACCOUNT => p612788289633-9a0tvq@gcp-sa-cloud-sql.iam.gserviceaccount.com
This allows service account to load data into GCS bucket 
Now to load the data, access cloud shell ahain and trigger gcloud command to export 
MYSQL query results to a csv file using shell script 

Now the script used is gcloudexportcloudsqltogcs.sh to breakdown the data 
export parts into different folders in gcs 
navigate to file and run the file 
sh filename.sh 
then finally delete mysql instances 
gcloud sql instances delete mysql-instance-source 


TASK 3: LOADING GCS TO BIGQUERY 
Bigquery console 
- create a new dataset named raw_bikesharing 
- click on table icon 
- create table from, choose gcs 
- browse and choose 20180101/stations.csv 
- for the table name select stations 
- schema - edit as text 
add to schema box  
station_id:STRING, name:STRING, region_id:STRING, capacity:INTEGER 
- create the table 
- check if data has been loaded into the table 


TASK 4: CREATE A BIGQUERY DATA MART 
DATA MARTS MOSTLY WHAT COMPANIES USE 
TO CREATE A DATAMART TWO WATS 
- CREATE A TABLE 
- CREATE A VIEW 
BOTH HAVE THEIR ADVANTAGES AND DISADVANTAGES 
VIEWS 
COSTS NO ADDITIONAL STORAGE 
DATA IS NOT STORAGE ANYWHERE BUT SAVED IN SQL FORMULA 
VIEW PULLS LATEST DATA CHANGES 

DEMERITS 
VIEWS CAN BE HEAVY AND WHEN MANY JOINS AND AGGREGATIONS IN THE VIEWS 
QUERY MAY END UP HAVING VERY HEAVY PROCESSING 

FOR INSTANCE IMAGINE HAVING 5 RAW TABLES WITH 1 PB EACH 
1000 VIEWS ACCESSING THE 5 TABLES 
END UP PROCESSING THAT HEAVY DATA REPEATEDLY AND THATS BAD FOR 
COST AND PERFORMANCE. 

BUT IN THIS EXAMPLE ITS SMALL SO LETS USE VIEW 
CREATE THE VIEW BASED ON THIS SCRIPT in the codebase 


DATAWAREHOUSE IN BIGQUERY - REQUIREMENTS FOR SCENARIO 2 
In the second scenario, load two more tables 
bike trips and regions 
Simulate how to handle data loading and thinking about data modelling 
1. As an operational business user 
- How many bike trips take place daily ?? 
- what is the daily average trip duration ?? 
- top five station names as the starting station that has the longest trips ? 
- The top five region names that have the shortest total trip durations? 

2. The bike trips data is in the GCS bucket, and each bucket folder contains daily data.

3. The regions data is from the BigQuery public dataset.

4. New data will be updated daily in the GCS bucket for stations and trips tables.

HOW TO TACKLE THE ISSUE 
1. WHAT WILL YOU DO ?? 
2. WHAT SERVICES WILL YOU USE ?? 
3. HOW WILL YOU DO THAT ?? 

We will use the python API instead of using the GCP console 

STEPS AND PLANNING FOR SCENARIO 2 
1. Since customer is intreasted in daily measurements, we will create a layer to 
provide daily aggregations to the user. 
2. There will be new data daily so we need to plan how to handle the incoming data 
from GCS directories. 
3. If you imagine three tables, stations, regions and trips in real life. 
they are different entities. 
Stations and regions are static objects 
Trips are events
Get a clearer information after checking the data 
we should think how to handle both types of information differently 

4. The data mart is similar to scenario 1. 
We can use the same datasets to store the result there. 

OVERALL STEPS ARE 
1. 20180101
LOAD EVENT FROM GCS TO BIGQUERY 
& 
LOAD SNAPSHOT TABLE FROM PUBLIC DATASET TO BIGQUERY 

2. 20180102 
BATCH LOAD EVENT TABLE 
& 
BATCH LOAD SNAPSHOT TABLE 

3. DATA MODELLING 

4. CREATE A DATA MART 

FIVE PRINCIPAL STEPS INVOLVED IN IT 
1. Create the required datasets 
2. Load the initial trips and region tables to BigQuery 
- trips from GCS 
- regions from the bigquery public datasets 
3. Handle the daily batch data loading 
- for the trips table 
- for the stations table 
4. design the data modelling for bigquery 
5. store the business questions result in the table 


STEP 1 
- CREATE THE DATASETS USING PYTHON 
Create a new dataset called dwh_bikesharing & 
raw_bikesharing & dm_bikesharing to the list. 
use file 
createdatasets.py or createdatasetsone.py 

STEP 2 INITIAL LOADING OF THE TRIPS TABLE INTO BIGQUERY 
The dataset is in GCS bucket inside directory dataset/trips 
use file loadtripsdatabigquery.py or loadtripsdatabigqueryone.py 
